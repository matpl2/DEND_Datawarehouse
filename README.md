# DEND Data Warehouse Project
This file is a documentation file for the exercise performed as a part of Data Engineer NanoDegree Project - Data Warehouse. 

Purpose of this project is to create analytical database that will be used by analytics team. Database chosen for this excercise is AWS Redshift. Redshift will be loaded with data from S3 buckets. These buckets consist data prodcued by dummy music streaming startup Sparkify.


## Files
This project includes following files:
 - dwh.cfg 
 - sql_queries.py 
 - create_tables.py
 - etl.py
 
Dwh.cfg is a configuration file that stores atributes needed for Redshift authorization and access to S3.
Sql_queries.py is a python file that stores SQL code for tables creation and for ETL process.
Create_tables.py and etl.py are python execution files that creates and load tables.


## Data sets
There are two datasets used in this project that reside in S3. Here are the S3 links for each:

Song data: s3://udacity-dend/song_data
Log data: s3://udacity-dend/log_data

Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 

Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

## Redshift instance
For the purpose of this project it is necessary to create Redshift database. Database creadentials, and server paths should be stored in dwh.cfg file. For my work I have created free trier 1 node Redshift instance that run in defalut VPC. In such config it was necessary to configure VPC components to get access to Redshift(editing Security group for public access, attaching subnets to IGW). 

Redshift instance used in this excercise:
![alt text](https://github.com/matpl2/DEND_Datawarehouse/blob/main/picts/redshift.png)
